# Technical Masterplan

### Abstract

The Open Voice Network is a non-profit industry association, operating as a Directed Fund of The Linux Foundation, and dedicated to the development of technical standards and ethical use guidelines. This is an essential step for a technology and an industry that is in its earliest days, and for which user value falls far short of envisioned potential.  

We believe that the future of AI-enabled voice will be found in a voice-first, conversation AI world that is multi-platform, multi-modal, multi-device, multi-use, and contextually integrated.  We also believe that the most significant barrier to the realization of user value in AI-enabled voice is the  lack of trust that users (from consumers to patients to enterprise decision-makers) have in current voice processes and providers. 

The Open Voice Network (OVN) Master Plan is a directional document with high-level design details that lays out the set of open standards needed to realize a trusted, open implementation for voice assistants and related services.

The content of this document is used to define the scope of what the OVN aims to work on and the sections below can map to working groups who will be creating the standards, TCKs, reference implementations, etc.

### Why the Open Voice Network?

Proposed standards will gain the greatest support (and eventual adoption) when such standards are developed through the leadership of an open, neutral, third party -- one dedicated to the best for the most.  The name "Open Voice Network" speaks to our two most important operating principles.  The first is that The OVN is "open" -- transparent in its work, and neutral in its interests.  The second is that The OVN is a network of voice user communities -- inclusive of aspirations and concerns from enterprises to voice developers and designers, data analysts and ethicists, marketers and media, and voice platforms worldwide.  

###  the Scope of Open Voice

We are Voice First.  We are language-centric, conversationally-focused, and cognizant of the variety and richness voice data.  We are also user-centric, inclusive (believing that voice is for all), and desirous of voice being a tool for societal good.

We recognize that voice will be increasingly combined with other digital modalities of communication and conversation, such as screen-based visuals, gestures, and facial recognition.  

However: to focus our work and drive results, we will solve voice first. 



The world of AI-enabled voice ranges from Interactive Voice Response (IVR) to the evolving realm of hardware-independent, voice-activated and -centric personal agents and assistants.  Over time, voice will be increasingly combined with other digital modalities of communication and conversation, including screen-based visuals, gestures, and facial recognition.  

Although Tomorrow, AI-enabled voice Although our definition of AI-enabled voice spans the entirety of this continuum, our primary focus will be on the integrated, pan-internet, and personal voice agent and assistant, which may include screen-based visual assistance.  

### Problem Statement 

### Why This Work Is Important ###

As we enter the third decade of this century, the ability to direct digital devices and systems through speech is increasingly common.
We speak (and listen) to voice assistants in purpose-built devices, smartphones, automobiles, televisions and home appliances, manufacturing systems, and enterprise software.   
However, as remarkable as today’s voice assistants appear, they represent only the earliest phase of what artificial intelligence-enabled voice will become by the end of this decade.  In the coming years, we can expect significant changes in three broad areas of voice assistance:  user experience, assistant availability, and the voice ecosystem.

The user experience of voice assistance will be transformed in multiple ways.  Of most importance will be the shift from today’s command-and-question interaction to human-to-machine conversation.  In practice, this will enable conversation akin to that with a friend, where shared history, time, place, sentiment, and personal preferences are brought seamlessly into the flow of a discussion.   A voice assistant will not only understand what is said, but the context in which it is said and the emotion with which it is said -- and be able to respond, recommend, and remember.  Voice will also be combined with visual elements, to provide what experts term “multi-modal” conversation.  This could range from information shown on screens to the recognition of gestures and facial behaviors. 

Voice will become available nearly everywhere -- the ubiquitous interface to the digital world. Voice assistance will be integrated not only into devices of all types, but into places (homes, stores, banks, medical facilities, museums, stadiums) and systems for monitoring environments and processes, from factories to farms.   Analysts predict active voice assistant use on more than two billion smartphones by 2023¹, and some 8.4 billion voice assistant users worldwide by 2024².   Users will search for and speak to websites (more than 1.7 billion worldwide today³), and smart, connected objects (some 41 billion worldwide by 2025⁴.)  Futurists see a world in which voice is the normative interface for all artificial intelligence; indeed, it is predicted that all artificial intelligence will be conversational.⁵

The voice assistance ecosystem will be transformed as well.  Of most importance will be the emergence of hundreds of thousands (and in time, millions) of independently-developed and purpose-built  enterprise and organizational voice assistants, each speaking and listening to constituents through a multitude of voice channels.   This will reflect a shift from platform-based skills and actions.

All told, we are today in the early days of the artificial intelligence-enabled voice revolution.  Today’s voice assistants offer but a glimpse of the potential value that this technology can bring to every industry, every profession, and society at large.

We – the sponsoring members and supporters of the Open Voice Network – believe that this glimpse of value is at risk of remaining faint and unfulfilled.   Standing before the growth of conversational AI in usage and economic value are four broad issues:

## Trust, choice, inclusivity, and openness.##

Trust opens the door to personal and enterprise adoption, and to ever-more complex usage by all users – from individuals to developers and designers to enterprise decision-makers.  The foundation of user trust is a “rule of law” which best emerges from the neutral, industry-driven development of standards and standards-based systems.   It is also built through the standards-based  protection of privacy, guidance for data use (both personal and commercial), inclusivity and accessibility, and system governance.  The Open Voice Network’s stated mission is to make voice worthy of user trust. 

Choice is a catalyst for ecosystem innovation and the growth of value for individuals, developers and designers, and enterprises of all sizes.  Choice is realized in many ways – in a user’s ownership and control of data, in a user’s choice of components, assistants and providers, and in the choice of assistant architecture (be it hosted locally or in the cloud.) 

Inclusivity is about everyone having a voice, and in every situation.  It is voice that welcomes under-represented vocalizations and dialects; voice that advances access. 

Openness enables interoperability among assistants and providers.  At its core is the identification and definition of standardized development building blocks.  Such blocks serve as a foundation for commercial differentiation and value; they can be assembled according to purpose, and in completely open, completely proprietary, or mixed (or hybrid) implementations. 


## The time to act is now.##

Artificial intelligence-enabled voice is now entering its next phase.

As such, we face the question of the future of voice assistance. 

Trust-worthy?  Choice-enabled?  Inclusive?  Open?

A realm of remarkable innovation, enterprise adoption, and economic growth that welcomes all on equal footing?  

## Introducing Open Voice Network.##

The Open Voice Network is a non-profit industry association, operating as a Directed Fund of the Linux Foundation, dedicated to researching and proposing the technical standards and usage guidelines that will bring the critical benefits of choice and trust to conversational AI.

We are neutral and inclusive, welcoming to participation and sponsorship all members of the conversational AI industry – from independent developers and designers to enterprise users to technology firms and platform providers.

We work openly, transparently, and communally, seeking ideas, comment, and criticism from a broad and diverse spectrum.

We work collaboratively with standards bodies and existing industry organizations.

We operate within a governance model pf checks and balances, one that demands accountability.

## Standards and Usage Guidelines: An Aspirational List.##

The Technical Advisory Council of The Open Voice Network is responsible for determining the OVN’s research and development of technical standards and usage guidelines. It reports to the Open Voice Network Steering Committee.

These aspirational topics are under discussion:

# Standards to enhance user interactions and capabilities#

Log-in procedures that are privacy-friendly, fast, and easy to use.
Platform-independent discovery and invocation of voice applications and services.
Platform-independent and dependable destination accessibility.
Seamless invocation of voice applications across platforms.
User interfaces that are consistent across human languages, dialects, and platforms.
User interfaces that effectively use the capabilities (microphones, speakers, displays, cameras, etc.) of end point devices.
## Standards that provide privacy, safety, and security.#
User identification and authentication that is multifactor and platform independent.
Intentional and consent-based individual and commercial privacy.
Object control language used to specify who may perform what function with which destination, and when.
## Usage guidelines that protect privacy and enhance inclusivity.#
Access that is unbiased, inclusive, and does not institutionalize racism, sexism, or ageism.
Transparent, intentional, and consent-based data usage.

Today, this world of voice assistance is dominated by proprietary, cloud-centric platforms (and related devices) that deliver services using closed implementations.  Examples include Amazon Alexa, Google Assistant, and Apple Siri.  While these closed systems have been successful, the lack of worldwide strategies and principles prevent them from becoming systems that work for all future voice participants.  Issues at question include:

- Deterministic privacy
- Platform and device interoperability
- Backend service choice
- Implementation sharing, reuse, and open improvement
- Enabling the consumer’s preference for the desired experience.

There are numerous other issues which, unless addressed through standards, will continue to inhibit the trust in and use of AI-enabled voice assistance, especially as the industry evolves toward its multi-platform future.   These include:
- system architecture as it relates to the analysis and storage of data
- platform-agnostic user identification and authentication
- the lack of common voice commands for common, non-differentiating business processes (such as transaction and payment)
- user inclusivity across languages, dialects, gender, and age.

### Vision of the Open Voice Network: our North Star

Conversational AI is in its earliest days.  It is rapidly moving toward a global future that will be multi-platform, multi-device,  multi-modal, and found in every industry.  It promises significant value not only to conversational AI providers, but to its users -- the enterprises, marketers and communicators, operational managers, developers and designers, and billions of individuals worldwide.

However, a broad issue of dis-trust now blankets conversational AI, and inhibits the realization of user value.  To realize its potential, conversational AI must be open and worthy of user trust.  It must be open for development and innovation.  It must be trust-worthy in its ease and reliability use, in its direct and unfiltered access to desired destinations, in its commercial and data usage and privacy, in its cross-platform interoperability, in lack of bias, in its development languages and protocols.  

Through the centuries, the development and adoption of standards has repeatedly catalyzed ever-advancing use, innovation, ecosystem development, and economic growth for new technologies.  We, the sponsors and supporters of The Open Voice Network, believe that now -- at the dawn of conversational AI's multi-platform future -- is the time to communally develop and drive toward adoption the conversational AI standards that will enable user trust.  

Our work could include the development and proposal of standards that will enable:
- platform-independent destination accessibility
- platform-indepedent user identification and authentication
- the interoperability of conversational AI platforms
- consent-driven commercial and consumer privacy; 
- commands and phraseology for common business processes;
- language inclusivity;
- open development. 

### Example Use Case(s) for Resolution

The Open Voice Network will test its proposed solutions against aspirational enterprise use cases from the vertical industries of the Open Voice Network:  commerce (retail and consumer goods), media, transportation, financial services, health and life sciences, public safety, connected cities, and education. 

Commerce:

The enterprise:  a multi-line retailer of goods and services with more than 50 million monthly active customers.  The customer base is 65% female, 35% male; speaks seven primary languages, with multiple dialects; and actively uses conversational AI to connect to the retailer through smartphones (40%), automobiles (30%), smart speakers (25%) and smart appliances (5%).  

Customers of this retailer use voice to establish and manage retailer club membership; place and pay for weekly replenishment orders; place and pay for unique orders; place and pay for prescription drug/pharmacy orders; place and pay for media and streaming services; manage orders and service scheduling-reservations with affiliated service providers; manage access real-time information as to order status and shipment; obtain household value information, ranging from recipes to beverage recommendations to new-born and infant care; obtain use and how-to information from branded product vendors. 

Customers will transact with this retailer using a wide range of payment options.  

The retailer considers all data within its customer interactions and transactions to be proprietary.   

### Solution

In the early days of the World Wide Web, user experience was shackled by closed implementations (e.g., AOL, etc).  Thankfully, open standards such as the Domain Name System (DNS) and industry-wide bodies such as the World Wide Web Consortium (W3C) unleashed the potential of the web, and it became the extraordinarily valuable and innovative ecosystem we know of today. So too can the future of voice-enabled devices and services also thrive from a set of open-standards that aim to improve privacy, interoperability, and backend choice -- all things that can unlock innovation, suitable for both users and providers.

Open standards should ideally cover all areas, wherein the absence of standards, an implementer would find themselves reinventing the wheel for things like (but not limited to):
Wake word implementations

- Different implementations understanding and handling conversations
- Dialog orchestration for dynamic responses
- Lack of privacy controls
- Registering voice assistants, operating them, and interoperability between them
- Commands for common business processes
- platform-agnostic identification and authentication- 

Also, there are new concepts needed to implement this in a way that's open, consistent and global:
Voice Registry Service (VRS)

- Dialog Brokers, Dialog Managers, and other new components
- Privacy controls and standard ways to communicate and disclose this to users
- 

Furthermore, like the open standards that made the WWW successful, these standards would aim to have these traits:

- Open development from and through a community of regional, language, and gender diversity;
- Open management by an open committee with a neutral governing entity
- Can have an open-source reference implementation
- Can be implemented as open-source and/or closed source products and services
- Abstracted at the right level as to not too deeply specify particular technologies used to implement them

## Principles


- Adhere to an open standard: due process, broad consensus, transparency, balance, and openness
- No one company or person sets the direction of these open standards
- Has the ability to protect the user's privacy and security, and the user can know where their privacy is protected or not
- Can be implemented as an open-source or closed source; implementations can be provided for free or not free
- Make voice inclusive and accessible by everyone
- Can be applied as low as localhost, a private network, or as big as globally provided

## Design

### General Vocabulary

This general vocabulary section aims to define general words used when talking about the use or design of voice assistants.

- **Artificial Intelligence (AI)** - also known as machine intelligence, is a type of computer science focused on designing intelligent computer systems that exhibit characteristics of human behavior. AI is an academic discipline that has multiple sub-fields such as Natural Language Processing, Neural Networks, Robotics, Speech Processing, Machine Learning.
- **Conversational AI** - is the set of technologies to enable automated communication between computers and humans. This communication can be speech and text. Conversational AI recognizes speech and text, understands intent, decipher various languages, and responds where it mimics human conversation. In some cases, it is also known as Natural Language Processing.
- **Utterance** - spoken or typed phrases.
- **Wake Word** - a specific word that will catch the attention of the channel.
- **Query** - user’s word requesting for specific function and expecting a particular response.
- **Automatic Speech Recognition** - is also known as Speech-To-Text or computer speech recognition. It is an interdisciplinary subfield of computer science and computational linguistics that enables the recognition and translation of auditory utterance to text.
- **Invocation** - is part of the construct of the user's utterance during a conversation with a channel. An invocation describes a specific function that the guest wants and expecting a particular response.
- **Voice Application** - also known as skill, action, capsule or domain. This is the specific executable component that has association to multiple things such as invocation, collection of related intents and entities up to the configuration to your dialog manager.
- **Platform** - The collection of components (the environment) needed to execute a voice application.  Examples of platforms include the Amazon and Google platforms that execute voice applications.
- **Component interoperability** - A component of a platform may be replaced by another component from a different vendor.  I.e., replace the TTS by another TTS from a different vendor.
- **Voice Application Interoperability** – a voice application involves another voice application


### Component Vocabulary

This component vocabulary section aims to briefly define the components involved with the design this paper proposes (see below in the document for more detailed explanations of each component).

- **Channel** - an interface and origin of communication. It is where the utterances are received. Examples: voice assistance devices, mobile phones, web sites, etc.
- **Text-To-Speech (TTS)** - is a text converting to audio. Also known as Automatic Speech Recognition. It includes customized models to overcome common speech recognition barriers, such as unique vocabularies, speaking styles, or background noise.
- **Natural Language Processing (NLP)** - a service and a branch of artificial intelligence that helps computers communicate with humans in their language and scales other language-related tasks. NLP helps structure highly complicated, unstructured human utterance and vice-versa. Natural Language Understanding is a subset of NLP that is responsible for understanding the meaning of the user's utterance and classifying it into proper intents.
- **Intent** - is a part of the structured machine translation. It is the identified action that the machine interprets based on the user's query. This is also known as classifiers.
- **Entity** - is a part of the structured machine translation. It is a custom level data type and considered a concrete value to associate a word in a query. This is also known as annotations.
- **Speech-To-Text (STT)** - is converting the response from an audio to a text.
- **Voice Registry System (VRS)** - is a global entity type in OVN and considered one of the most central components. It is a registry system with similarities to Domain Name System (DNS), but for voice. VRS resolves requests to dialog management endpoints, NLP providers, and the dialog broker. VRS serves consistently regardless of the NLP.
- **Dialog Broker (DB)** - is responsible for providing the fulfillable intents available for a resolved VRS record (e.g. where resolved VRS record "BigGrocery", it’s fulfillable intents might be "order product, check order status, add to shopping list". These fulfillable intents can execute remotely on the DM or download locally on the device.
- **Dialog Manager (DM)** - handles the dynamic response of the conversation. It provides a more personalized response based on the action provided by the NLP to send back to the user.

## Why are new and additional voice standards needed?

Broadly stated, voice has evolved over the past decade from call center-centric recognition to a consumer-owned, platform-enabled all-purpose answer machine. In its next phase – one that is unfolding now – it will become the enabler of contextually-aware dialogue and true personalized assistance.
The next step in the voice standards journey is address the transitions noted above, as well as these more recent evolutions: • Rapid advances in the AI-enabled voice ecosystem and value chain. A unique industry is now being created with independent voice assistance by world-leading technology firms. Voice is increasingly the entry point of a data-consuming engine of technology and economic growth. Today’s voice questions are different, and much larger than before.

• Ongoing advances in AI-enabled voice technology. Several of the standards listed below were developed to enable interactive voice response systems for telephone call centers. We now stand at the edge of contextually-aware voice functionality that enables human-like dialogue.

• The emergence of platform-independent voice assistants. This calls into question platform and assistant interoperability, in which an enterprise-owned independent assistant can speak with all others, regardless of platform or channel. It suggests a standardization of components, the building blocks of voice assistants – components that have emerged from many of the standards listed below.

As the OVN begins its work, it does so with the desire to learn from and build upon a significant number of widely-adopted and currently-relevant standards for voice assistance. The list below identifies existing voice standards that may guide the functionality and definition of voice assistant components (Dialogue Manager, TTS, ASR, NLU, and many others) and governance protocols (a potential destination registry) now in development by the OVN Technical Committee. It is acknowledged that this list may not be comprehensive.

## Existing (Q4 2020) Standards That May Have Relevance to the Work of the OVN 

This list was assembled by Open Voice Network supporters and advisors Dr. Deborah Dahl, Dr. Jim Larson, David Attwater, and Jonathan Eisenzopf. We are deeply grateful for their contributions and ongoing guidance.

Component Potentially related standards
IPA Client • (X)HTML, specifies visual content and presentation of web pages IPA Service none Dialog Management • Voice Extensible Markup Language (VoiceXML) 2.1 designed for creating audio dialogs that feature synthesized speech, digitized audio, recognition of spoken and DTMF key input, recording of spoken input, telephony, and mixed initiative conversations. Its major goal is to bring the advantages of Web-based development and content delivery to interactive voice response applications. • State Chart XML (SCXML) provides a generic state-machine based execution environment based on CCXML and Harel State Tables. May be used to specify dialogs

TTS • Web Speech API, a JavaScript API to enable web developers to incorporate speech recognition and synthesis into their web pages. It enables developers to use scripting to generate text-to-speech output and to use speech recognition as an input for forms, continuous dictation and control. • Speech Synthesis Markup Language (SSML) Version 1.0 a standard way to control presentation of speech such as pronunciation, volume, pitch, rate, etc. across different synthesis-capable platforms. • Pronunciation Lexicon Specification Version 1.0 syntax for specifying pronunciation lexicons to be used by Automatic Speech Recognition and Speech Synthesis engines in voice browser applications. • Emotion Markup Language (EmotionML) 1.0 as a "plug-in" language suitable for use in three different areas: (1) manual annotation of data; (2) automatic recognition of emotion-related states from user behavior; and (3) generation of emotion-related system behavior. • ToBI, a set of conventions for transcribing and annotating the prosody of speech

ASR • Web Speech API , a JavaScript API to enable web developers to incorporate speech recognition and synthesis into their web pages. It enables developers to use scripting to generate text-to-speech output and to use speech recognition as an input for forms, continuous dictation and control. • Speech Recognition Grammar Specification Version 1.0, syntax for representing grammars for use in speech recognition so that developers can specify the words and patterns of words to be listened for by a speech recognizer. • Pronunciation Lexicon Specification Version 1.0 syntax for specifying pronunciation lexicons to be used by Automatic Speech Recognition and Speech Synthesis engines in voice browser applications. • Semantic Interpretation for Speech Recognition (SISR) Version 1.0 process of Semantic Interpretation for Speech Recognition and the syntax and semantics of semantic interpretation tags that can be added to speech recognition grammars to compute information to return to an application on the basis of rules and tokens that were matched by the speech recognizer. In particular, it defines the syntax and semantics of the contents of Tags in the Speech Recognition Grammar Specification • International Phonetic Alphabet – maybe too much detail but often used to represent the sound of words in lexicons.

Core Dialog and Annotation • Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech Acts (DAMSL) a statistical approach for modeling dialogue acts in conversational speech • ISO 24617-2 also known as DIT++ or DiAML and background can be found at http://www2.lpl-aix.fr/~otim/documents/OTIM_HarryBunt_Wshop24mai2011.pdf • There are 4-5 primary ways dialog managers work, but there are no real standards yet. Predominant methods are Answerset Logic (Q&A) and Finite State Machines with Slot filling (used in VoiceXML, Alexa, Google Assistant, Watson, …). Newer methods that will be introduced in the coming years are Information State Update (Traum et.al) and Incrementation Dialog Processing (Gabriel Skantze, David Schlangen). • The text encoding initiative - https://tei-c.org/ a set of guidelines regarding XML markup of human computer interaction data. It covers many areas but those relevant to dialogs are : Metadata for language corpora, representing certainty and uncertainty, definition of underlying feature annotation structures (e.g. data structures representing the internal structure of a language feature including key value pairs, trees, graphs), orthographic regularization, transcribed speech, phrasal boundary notation, cross-reference, segmentation and alignment of text or other feature sequences facilitating multi-layered stand-off annotation.

Core Intent Set • Discourse.ai may contribute our JSON specification

Dialog Registry • Discovery & Registration of Multimodal Modality Components a standardized way to build a web Application that can dynamically combine and control discovered components by querying a registry build based on the multimodal types of the modalities and their states.

Provider Selection Service • The Open Agent Architecture is not fully define but is used internally for Viv and Siri

Accounts -Authentication • Web Authentication an API enabling the creation and use of strong, attested, scoped, public key-based credentials by web applications, for the purpose of strongly authenticating users • IDO Universal Authentication Framework WIth FIDO UAF , the user carries a device with a FIDO UAF stack installed. They can then register their device to the online service by selecting a local authentication mechanism such as swiping a finger, looking at the camera, speaking into the mic, entering a PIN, etc. The FIDO UAF protocol allows the service to select which mechanisms are presented to the user. • Oauth – this is what Amazon and Google use currently. • OpenID will likely replace Oauth and SAML in the future

Core NLU • EMMA: Extensible MultiModal Annotation markup language Version 2.0 set of specifications for multimodal systems providing details of an XML markup language for containing and annotating the interpretation of user input and production of system output. • JSON Representation of Semantic Information a JSON format for representing the results of semantic processing • Abstract Meaning Representation used for semantic representation language. Is a culmination of prior researchers and projects and could become how semantic meaning of utterances is represented in the future. • ARPA N-Gram format a published format to portably represent language models in N-Gram format.

World Knowledge • Web Ontology Language (OWL) a Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. • Resource Description Framework (RDF) a framework for representing information in the Web • Schema.org, an OWL ontology used to organize all information on the web.

Data Provider none
17 November 2020 Standards listing from Dahl, Larson, Eisenzopf, and Attwater. Introductory text, Stine.





## Examples

Several examples that are useful to explain all the concepts required in the design:

"Computer, please add milk to my shopping list at BigGrocery"

- machine translates to → {wakeword}, {query} {vrs}
- intent: addShoppingList
- entities: milk = @product

"Computer, ask BigGrocery to add milk to my shopping list"

- translates to → {wakeword}, {vrs} {query}
- intent: addShoppingList
- entities: milk = @product

## Component Architecture

![](https://raw.githubusercontent.com/open-voice-network/docs/master/technical_masterplan_assets/component_architecture_diagram.png?token=AABSWQTW7TMH2VHUFSG5Q226UH7XC "Fig. 1 - Component Architecture Diagram")

Note: Dialog Broker, VRS, and Dialog Manager are new concepts. NLP, TTS / STT, and Channels are things that already exist, but we list them because they will be affected and influenced by the standards.

## Component Flow

This component flow section aims to describe the general steps that happen in and between each component.

![](https://raw.githubusercontent.com/open-voice-network/docs/master/technical_masterplan_assets/component_flow_sequence_diagram.png?token=AABSWQX53UKQK2Y3A557URK6UIA4A "Fig. 2 - Component Flow Sequence Diagram")

1. "{wake word}, add milk to my shopping list at BigGrocery."
2. NLP calls VRS and passes the query. NLP sends in the query its location and what it thinks the VRS name lookup should resolve for (in the case below it understands that "biggrocery" is what the VRS lookup is for). See Figure 1.0.

```
{
  "query": "add milk to my shopping list at biggrocery",
  "vrs_name_lookups": [
    { "vrs_name": "biggrocery"}
  ],
  "channel_location": {
    "lat": "",
    "long": ""
  },
  "channel": "app",
  "source_nlp_provider": "AmazonLex"
}
```

3. The name lookup of the "biggrocery" is resolved in VRS. It will respond with the records for the dialog broker, dialog manager, and NLP for the given location(s) that were requested in the lookup. See Figure 2.0 and Figure 2.1.
4. NLP will call the Dialog Broker based on the endpoint it receives from the VRS. See Figure 3.0
5. NLP identifies the intent and entities. If identified intent is not part of the fulfillable intents, NLP will do a default fail response.

```
{
  "intent": "addShoppingList",
  "entities": [
    { "@product" }
  ],
  "query": "add milk to my list at biggrocery"
}
```

6. If VRS returns the Dialog Manager endpoint, NLP will call the Dialog Manager to have a better dynamic response for the vendor.

## Component Details

This component details section aims to further define the individual components in more detail.

### Utterance

An utterance construct is: {wake word}, {query}

### Wake Word

Wake word options:

- Static wake word
- Trainable static wake word
- Custom trainable wake word (e.g. instead of "Hey Google", "Hey Johnny")
- Add another ecosystem’s wake word (Hey Google, Hey Alexa, Hey Siri, etc) to a device in addition to the wake word(s) already on the device (this would result in more than 1 wake word being on the device). This would require a standard given it’s an interface that would need standardization.

### Query

The words that the user is using after the wake word. The query can have multiple constructs.

Examples:

- {invocation}
- {launch phrase} {explicit invocation}
- {launch phrase} {implicit invocation}

### Channel

Channel is the physical or virtual interface to the user where the dialog between the computer and the user occurs (e.g. a physical device, a web page, apps, etc). It is responsible for receiving voice input from a user and delivering back a potentially personalized response using inputs from TTS and the Dialog Manager. It manages the specific native framework such as cards to provide a better user experience and address some visual nuisances to individual channels.

### STT / TTS

Speech To Text (STT) is the component of the architecture that converts the audio to text. It applies a deep-learning AI algorithm to apply knowledge about noise robustness, phrase hints, spelling, language structure, and global vocabulary of any utterances.

Text To Speech (TTS) is the opposite of the STT. It translates a text to a more synthesized natural-sounding speech. The TTS is the component that creates a simulated conversation with the user.

Both STT, and TTS can be implemented on the cloud, on-premise, or embedded devices based on what the technology offers.

There are multiple known STT / TTS technologies in the industry such as Watson STT, Google Cloud, Microsoft, and Mozilla Deepspeech.

This is another domain where OVN can play a significant role in influencing the standard schema for all the STT / TTS technologies players.

### NLP

As we enter the era of human-machine conversation, the crown jewel component of it is Natural Language Processing. The problem we are facing today is the lack of standards from different NLP providers. In the world of open-standard, we are introducing the concept of a global entity, i.e., @vrsname. It is not about primitive data types that individual NLP understands but a smart object that all NLP providers understand. In the pre-processing stage of NLP, it is evaluating the query as a whole and identifying if any global identity exists in the query. The identified global entity is submitted to the VRS.

The global identity solves the interoperability aspect of NLPs. For example, the word "BigGrocery" in Amazon NLP is going to be treated the same way in Microsoft Luis or Einstein. Like domain names, this gives businesses the flexibility to be handled identically in the world of the human-machine conversation regardless of the NLP provider they choose.

NLP schema is another place where OVN can influence the standard. Different NLPs have their own set of schema that leads to the different implementation of standards. Some NLPs understand the concept of entity, but some do not. Some understand the intent, and some have different flavors of it and call it "action." Because of the closed implementations, each business has to orchestrate this separately or build their abstracted implementation version to be able to adapt to different NLP standards. OVN is proposing to set a standard schema for all NLP to follow:

TODO: below is WIP / invalid JSON

```
{
  "response_id": UUID,
  "query": "add one milk to my shopping list at biggrocery", // raw version of the user inquiry
  "query_result": {
    "intents": [ // list of intents related to the query
      {
        "name": "addShoppingList",
        "display_name": "add shopping list",
        "confidence_score": 0.9999,
        "is_top_score": "true"
      },
      {
        "name": "orderList",
        "display_name":"order shopping"
        "confidence_score": 0.7982
        "entities": [ //list of entities related to the query
     {
       "one": "sys.integer"
     },
     {
       "milk": "@product"
     }
],
"is_required_parameters_present": "true", //boolean value if all required parameters in the top intent are supplied
"history_context":
  {
    …
  } //array of intents and entities from previous intents.
"response": [ //array of intents and entities from previous intents
  {
    "text": "one milk is added to your shopping list at BigGrocery",
    "default_response": ""
  }
],
    }
  "vrs_name_lookups": [
    {
      "vrs_name": "biggrocery"
    }
  ],
  "channel_location": {
    "lat": "",
    "long": ""
  },
  "channel": "app",
  "source_nlp_provider": "microsoft",
  "language": "en"
}
```

### Voice Registry System

Voice Registry System (VRS) is the new concept that is part of the open-voice standards. VRS is a new global entity and, therefore, part of the pre-processing identification in the NLP. Once the NLP identifies @vrs, it passes the information to the VRS server.

The job of the VRS is to resolve the query by trying to resolve first in the root server. If not found, it is smart enough to point to the next name server, and this process continues until the request is resolved, or determined that the requested @vrs is not registered.

If VRS server is able to resolve the query, it returns a JSON response of:

```
{
  "vrs_id": UUID,
  "query": "add milk to my shopping list at biggrocery",
  "vrs_name_lookup": "biggrocery",
  "is_registered": "true",
  "locations": [
    {
      "country": "USA",
      "state": "",
      "city": "",
      "neighborhood":"",
      "dialog_manager": {
        "dialog_manager_id": UUID,
        "url": "biggrocery.com/dialog_manager"
      },
      "nlps": [
        {
          "nlp_id": UUID,
          "url": "biggrocery.com/nlp"
        }
      ],
      "dialog_broker":  {
        "dialog_broker_id": UUID,
        "url": "biggrocery.com/dialog_broker"
      }
    }
  ]
}
```

If not found, it returns a JSON response of:

```
{
  "vrs_id": UUID,
  "query": "add milk to my shopping list at biggrocery",
  "vrs_name_lookup": "biggrocery",
  "is_registered": "false",
  "locations": []
}
```

### Dialog Broker

The Dialog Broker provides a list of fulfillable intents.

```
{
  "dialog_broker_id": UUID,
  "url": "biggrocery.com/dialog_broker",
  "fulfillable_intents": [
     # some object(s) here that defined a fulfillable intent
     # add to shopping list
     # browse products
     # etc
  }
}
```

TODO: add more details here

### Dialog Manager

Manages the actual context of the conversation. It is responsible for choosing the best action to perform the conversation.

```
{
  "response_id": UUID,
  "query_result": {
	...
   }
}
```

### Channel Registry

TODO: add introduction and more details here; add to diagrams and flows above

TODO: clean up this bulleted list; it’s just a placeholder of things to consider for now.

- Channel registry should work be implementable on any private network OR be a SaaS service provided by a provider
- Each channel can be a part of a channel registry
- A channel in a channel registry should be able to be made aware of other channels registered in the channel registry
- A channel fetches its configuration from the channel registry
- Configuration in the channel registry should specify a standard for mandatory configuration items, optional configuration items, or allow flexibility for user-defined configuration items (ex. kind of like how HTTP headers are)
- Optional configuration items:
- SST engine to be used by the channel

## Vertical Dependencies
Items in the vertical dependencies need to be addressed in each architectural component.

# Privacy
Part of the principles of OVN is privacy. Below are the guidelines to focus on:
- Always end-users interest first. This is essential, especially for end-users who are marginalized or vulnerable sectors of the society who may not be aware of how their data are being collected, used, or shared. 
- Always ask. Get an informed acknowledgment from the end-user, before collecting, sharing, or using the data.
- Provide an audit trail for personal or sensitive information captured.
- Always communicate to the end-user the data being collected and informed the purpose of the collection of data.

# Security
Add

# Context


## Addendums

TODO

### Visual Displays

TODO



### Common Commands for Common Processes

TODO

### Platform-Agnostic User Identification and Authentication

TODO

### Platform Interoperability


## References

- Fig. 1 - Component Architecture Diagram: https://docs.google.com/drawings/d/1ELxRKqyaUCWRoc5ein_ajHsy3Y_ZDpY9ErIfGT0lYq4/edit?usp=sharing
- Fig. 2 - Component Flow Sequence Diagram: https://docs.google.com/drawings/d/1rMfX6-oN81t27WD6Dbkd6Bee1KDYkHWEXPb-tNn0ykQ/edit?usp=sharing 


